# -*- coding: utf-8 -*-
"""std.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12NsqbDacDQ2-_JZJJZCDD73PD5gayntG
"""

import numpy as np
import pandas as pd
from scipy import stats

# Existing model accuracies
transnn_mha_accuracies = [0.9094, 0.9106, 0.9280, 0.9331, 0.9466, 0.9156]
cnn_transformer_accuracies = [0.8807, 0.8634, 0.8671, 0.8422, 0.8807, 0.8298]
gru_transformer_accuracies = [0.9106, 0.9043, 0.9317, 0.9006, 0.9304, 0.8957]

# New model accuracies
deepconv_transformer_accuracies = [0.7797, 0.7525, 0.8193, 0.7373, 0.8178, 0.8042]
self_attention_accuracies = [0.2000, 0.2000, 0.2000, 0.2012, 0.2012, 0.2000]
spatial_temporal_transformer_accuracies = [0.7975, 0.8186, 0.8422, 0.8037, 0.8211, 0.7938]
transformer_gnn_accuracies = [0.88, 0.8832, 0.9130, 0.8720, 0.9180, 0.8708]
ts_transformer_accuracies = [0.3627, 0.4335, 0.4820, 0.3665, 0.3640, 0.3888]
hierarchical_transformer_accuracies = [0.7752, 0.8062, 0.8273, 0.7839, 0.8012, 0.7801]
lstm_attention_accuracies = [0.5466, 0.5255, 0.5242, 0.5304, 0.5317, 0.5627]
ts_bert_accuracies = [0.2000, 0.2000, 0.2000, 0.2012, 0.2012, 0.2000]

# Calculate means and standard deviations for all models
model_names = [
    'TransNN-MHA', 'CNN-Transformer', 'GRU-Transformer', 'DeepConv + Transformer',
    'Self-Attention', 'Spatial-Temporal Transformer', 'Transformer + GNN',
    'TS-Transformer', 'Hierarchical Transformer', 'LSTM + Attention', 'TS-BERT'
]

model_accuracies = [
    transnn_mha_accuracies, cnn_transformer_accuracies, gru_transformer_accuracies,
    deepconv_transformer_accuracies, self_attention_accuracies, spatial_temporal_transformer_accuracies,
    transformer_gnn_accuracies, ts_transformer_accuracies, hierarchical_transformer_accuracies,
    lstm_attention_accuracies, ts_bert_accuracies
]

# Calculate mean and standard deviation for each model
means = [np.mean(acc) for acc in model_accuracies]
stds = [np.std(acc) for acc in model_accuracies]

# Prepare a summary table
df_summary = pd.DataFrame({
    'Model': model_names,
    'Mean Accuracy': means,
    'Standard Deviation': stds
})

# Kruskal-Wallis test between all models
kruskal_test = stats.kruskal(*model_accuracies)

# Display the summary of model performance
print("Model Performance Summary:\n", df_summary)

# Return Kruskal-Wallis test results
print("\nKruskal-Wallis Test Result:", kruskal_test)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages

# Confusion matrices for each model
conf_matrix_gnn = np.array([[846, 41, 34, 36, 9],
                            [43, 842, 33, 28, 21],
                            [33, 27, 859, 43, 2],
                            [38, 29, 43, 841, 13],
                            [13, 18, 13, 15, 910]])

conf_matrix_gru = np.array([[873, 29, 27, 26, 11],
                            [40, 850, 27, 35, 15],
                            [24, 20, 881, 37, 2],
                            [32, 29, 32, 862, 9],
                            [14, 7, 3, 5, 940]])

conf_matrix_transnn = np.array([[875, 26, 38, 19, 8],
                                [33, 858, 26, 40, 10],
                                [23, 20, 868, 48, 5],
                                [25, 33, 36, 860, 10],
                                [4, 13, 12, 3, 937]])

# Labels for the classes
class_labels = ['Imaginary Both Feet', 'Imaginary Right Fist', 'Real Both Feet', 'Real Right Fist', 'Rest']

# Function to plot a confusion matrix with custom formatting
def plot_confusion_matrix(cm, title, ax):
    sns.heatmap(cm, annot=True, fmt="d", cmap="tab20", ax=ax, annot_kws={"size": 25})
    ax.set_xlabel("Predicted Labels", fontsize=25)
    ax.set_ylabel("True Labels", fontsize=25)
    ax.set_title(title, fontsize=25)
    ax.tick_params(axis='both', which='major', labelsize=15)

# Save and show plots
def save_and_show_plots(confusion_matrices, models, filename):
    with PdfPages(filename) as pdf:
        n_rows = (len(models) + 2) // 3  # Calculate number of rows needed
        fig, axes = plt.subplots(n_rows, 3, figsize=(20, 5 * n_rows))  # Adjust figure size
        axes = axes.flatten()  # Flatten the axes array for easy indexing

        for ax in axes[len(models):]:  # Hide any unused axes
            ax.axis('off')

        for ax, model in zip(axes, models):
            plot_confusion_matrix(confusion_matrices[model], f"{model}", ax)

        plt.tight_layout(pad=3.0)
        pdf.savefig(fig)  # Save the figure to PDF
        plt.show()

# Models and their confusion matrices
confusion_matrices = {
    "Transformer + GNN": conf_matrix_gnn,
    "GRU-Transformer": conf_matrix_gru,
    "TransNN-MHA": conf_matrix_transnn
}

# Plot and save the confusion matrices to a PDF
models = ["Transformer + GNN", "GRU-Transformer", "TransNN-MHA"]
save_and_show_plots(confusion_matrices, models, "confusion_matrices.pdf")

import numpy as np
# Set print options to avoid scientific notation
np.set_printoptions(suppress=True)
# Angles in radians
theta_1 = np.pi / 2
theta_2 = -np.pi / 4
theta_3 = 0
theta_4 = np.pi / 4

# Skew-symmetric matrices
z_hat_bracket = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 0]])
y_hat_bracket = np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]])

# Rodrigues' formula to compute the exponentials
def rodrigues_formula(hat_matrix, theta):
    return np.eye(3) + np.sin(theta) * hat_matrix + (1 - np.cos(theta)) * np.dot(hat_matrix, hat_matrix)

# Calculate the rotation matrices
e_z_hat_bracket_theta_1 = rodrigues_formula(z_hat_bracket, theta_1)
e_y_hat_bracket_theta_2 = rodrigues_formula(y_hat_bracket, theta_2)
e_y_hat_bracket_theta_3 = rodrigues_formula(y_hat_bracket, theta_3)
e_y_hat_bracket_theta_4 = rodrigues_formula(y_hat_bracket, theta_4)

# Calculate the final rotation matrix (product of exponentials)
R = np.dot(np.dot(np.dot(e_z_hat_bracket_theta_1, e_y_hat_bracket_theta_2), e_y_hat_bracket_theta_3), e_y_hat_bracket_theta_4)

# Print the final rotation matrix
print("Final Rotation Matrix R:")
print(R)

import sympy as sp

# Define symbolic variables for joint angles
t1, t2, t3, t4 = sp.symbols('t1 t2 t3 t4')

# Define the skew-symmetric matrices for symbolic computation
z_hat_bracket = sp.Matrix([[0, -1, 0], [1, 0, 0], [0, 0, 0]])
y_hat_bracket = sp.Matrix([[0, 0, 1], [0, 0, 0], [-1, 0, 0]])

# Rodrigues' formula for symbolic computation
def rodrigues_formula_sym(hat_matrix, theta):
    return sp.eye(3) + sp.sin(theta) * hat_matrix + (1 - sp.cos(theta)) * (hat_matrix * hat_matrix)

# Compute the exponentials
e_bracket_z_t1 = rodrigues_formula_sym(z_hat_bracket, t1)
e_bracket_y_t2 = rodrigues_formula_sym(y_hat_bracket, t2)
e_bracket_y_t3 = rodrigues_formula_sym(y_hat_bracket, t3)
e_bracket_y_t4 = rodrigues_formula_sym(y_hat_bracket, t4)

# Multiply the exponentials to get the final orientation matrix
result = e_bracket_z_t1 @ e_bracket_y_t2 @ e_bracket_y_t3 @ e_bracket_y_t4

# Simplify the result
simplified_result = sp.simplify(result)

# Display the simplified result
sp.pprint(simplified_result)

# Now equate the result to the desired orientation and solve for the joint angles
desired_orientation = sp.Matrix([[-1, 0, 0], [0, 0, -1], [0, -1, 0]])

# Solve the system of equations
solution = sp.solve(sp.Eq(simplified_result, desired_orientation), [t1, t2, t3, t4])

# Print the solution
print("Joint angles that achieve the desired orientation:")
print(solution)