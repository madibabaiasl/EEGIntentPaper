{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0cf25-bc63-40dd-bebc-d2b1426ca0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('augmented_data.csv')\n",
    "\n",
    "# Remove rows where all column values are duplicated\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Preprocessing\n",
    "features = data.drop(columns=['Target'])\n",
    "target = data['Target']\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(target)\n",
    "\n",
    "# Reshape data for feature selection\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Initialize an empty score array\n",
    "scores = np.zeros(X_resampled.shape[1])\n",
    "\n",
    "# 1. Mutual Information\n",
    "mi = mutual_info_classif(X_resampled, y_resampled)\n",
    "scores += mi\n",
    "\n",
    "# 2. ANOVA F-test\n",
    "f_test, _ = f_classif(X_resampled, y_resampled)\n",
    "scores += f_test\n",
    "\n",
    "# 3. Recursive Feature Elimination (RFE)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "rfe = RFE(model, n_features_to_select=30)\n",
    "rfe.fit(X_resampled, y_resampled)\n",
    "rfe_scores = np.array([1 if i in rfe.support_ else 0 for i in range(X_resampled.shape[1])])\n",
    "scores += rfe_scores\n",
    "\n",
    "# 4. L1 Regularization (Lasso)\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=500)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "lasso_scores = np.abs(model.coef_[0])\n",
    "scores += lasso_scores\n",
    "\n",
    "# 5. Tree-based Feature Importance\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "tree_scores = model.feature_importances_\n",
    "scores += tree_scores\n",
    "\n",
    "# Average the scores\n",
    "average_scores = scores / 5\n",
    "\n",
    "# Select the top 100 features\n",
    "top_100_indices = np.argsort(average_scores)[-70:]\n",
    "X_selected = X[:, top_100_indices]\n",
    "\n",
    "# Reshape the selected features for TS-Transformer input\n",
    "X_selected_reshaped = X_selected.reshape(X_selected.shape[0], X_selected.shape[1], 1)\n",
    "\n",
    "# Cross-Validation with the selected features\n",
    "n_splits = 6\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "classification_reports = []\n",
    "\n",
    "# Transformer-based model adapted for time-series data (TS-Transformer)\n",
    "def build_ts_transformer_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Positional Encoding for Time Series Data\n",
    "    x = layers.Conv1D(filters=64, kernel_size=1, activation='relu')(inputs)\n",
    "\n",
    "    # Transformer Encoder\n",
    "    for _ in range(4):  # Adjust the number of layers as needed\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.2)(x, x)\n",
    "        x = layers.Add()([attention_output, x])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Training with cross-validation\n",
    "for train_index, val_index in skf.split(X_selected_reshaped, y):\n",
    "    print(f\"Training on fold {fold}...\")\n",
    "\n",
    "    X_train, X_val = X_selected_reshaped[train_index], X_selected_reshaped[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_ts_transformer_model(input_shape=(X_train.shape[1], X_train.shape[2]), num_classes=len(np.unique(y)))\n",
    "    model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=3e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=100,\n",
    "              batch_size=128,\n",
    "              callbacks=[early_stopping, reduce_lr],\n",
    "              verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='macro')\n",
    "    recall = recall_score(y_val, y_pred, average='macro')\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    report = classification_report(y_val, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    classification_reports.append(report)\n",
    "\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold} Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Summary of Cross-Validation Results\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print summary results\n",
    "print(f\"\\nMean Cross-Validation Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Mean Cross-Validation Precision: {mean_precision:.4f}\")\n",
    "print(f\"Mean Cross-Validation Recall: {mean_recall:.4f}\")\n",
    "print(f\"Mean Cross-Validation F1-Score: {mean_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
